# Open-Source-ChatGPT

Meta has recently released LLaMA, a collection of foundational large language models ranging from 7 to 65 billion parameters. LLaMA is creating a lot of excitement because it is smaller than GPT-3 but has better performance.

However, LLaMA was not fine-tuned for instruction task with a Reinforcement Learning from Human Feedback (RLHF) training process.

An open source implementation for LLaMA-based ChatGPT: https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama

## LLaMA: INT8 edition
https://github.com/tloen/llama-int8

## Open distribution of LLaMA (llama-dl)
https://github.com/shawwn/llama-dl

## RWKV: RNN with Transformer-level LLM Performance

RWKV is a RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).

https://github.com/BlinkDL/RWKV-LM
